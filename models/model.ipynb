{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "# from skimage import morphology\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "MODEL_DIR = os.getcwd()\n",
    "ROOT_DIR = os.path.dirname(MODEL_DIR)\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "sys.path.append(MODEL_DIR)\n",
    "from block import res_path, res_block, decoder_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransResUNet(input_size=(512, 512, 3)):\n",
    "    \"\"\"\n",
    "    TransResUNet -- main architecture of TransResUNet\n",
    "    \n",
    "    Arguments:\n",
    "    input_size {tuple} -- size of input image\n",
    "    \n",
    "    Returns:\n",
    "    model {<class 'tensorflow.python.keras.engine.training.Model'>} -- final model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input \n",
    "    inputs = Input(input_size)\n",
    "    inp = inputs\n",
    "    input_shape = input_size\n",
    "    \n",
    "    # Handling input channels \n",
    "    # input with 1 channel will be converted to 3 channels to be compatible with VGG16 pretrained encoder \n",
    "    if input_size[-1] < 3:\n",
    "        inp = Conv2D(3, 1)(inputs)                         \n",
    "        input_shape = (input_size[0], input_size[0], 3)  \n",
    "    else:\n",
    "        inp = inputs\n",
    "        input_shape = input_size\n",
    "\n",
    "    # VGG16 with imagenet weights\n",
    "    encoder = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "       \n",
    "    # First encoder block\n",
    "    enc1 = encoder.get_layer(name='block1_conv1')(inp)\n",
    "    enc1 = encoder.get_layer(name='block1_conv2')(enc1)\n",
    "    # Second encoder block\n",
    "    enc2 = MaxPooling2D(pool_size=(2, 2))(enc1)\n",
    "    enc2 = encoder.get_layer(name='block2_conv1')(enc2)\n",
    "    enc2 = encoder.get_layer(name='block2_conv2')(enc2)\n",
    "    # Third encoder block\n",
    "    enc3 = MaxPooling2D(pool_size=(2, 2))(enc2)\n",
    "    enc3 = encoder.get_layer(name='block3_conv1')(enc3)\n",
    "    enc3 = encoder.get_layer(name='block3_conv2')(enc3)\n",
    "    enc3 = encoder.get_layer(name='block3_conv3')(enc3)\n",
    "\n",
    "    # Center block\n",
    "    center = MaxPooling2D(pool_size=(2, 2))(enc3)\n",
    "    center = decoder_block(center, 512, 256)\n",
    "\n",
    "    # Decoder block corresponding to third encoder\n",
    "    res_path3 = res_path(enc3,128,3)\n",
    "    dec3 = concatenate([res_path3, center], axis=3)\n",
    "    dec3 = decoder_block(dec3, 256, 64)\n",
    "    # Decoder block corresponding to second encoder\n",
    "    res_path2 = res_path(enc2,64,2)\n",
    "    dec2 = concatenate([res_path2, dec3], axis=3)\n",
    "    dec2 = decoder_block(dec2, 128, 64)\n",
    "    # Final Block concatenation with first encoded feature \n",
    "    res_path1 = res_path(enc1,32,1)\n",
    "    dec1 = concatenate([res_path1, dec2], axis=3)\n",
    "    dec1 = Conv2D(32, 3, padding='same', kernel_initializer='he_normal')(dec1)\n",
    "    dec1 = ReLU()(dec1)\n",
    "    out = Conv2D(2, (1, 1), padding='same')(dec1)\n",
    "    # Final model\n",
    "    model = Model(inputs=[inputs], outputs=[out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransResUNet()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dataset\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(DATA_DIR, \"train\", \"img\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(DATA_DIR, \"train\", \"mask\"), exist_ok=True)\n",
    "os.makedirs(TEST_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(DATA_DIR, \"test\", \"img\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(DATA_DIR, \"test\", \"mask\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MASK_DIR = os.path.join(DATA_DIR, \"JSRT\", \"scr\", \"masks\")\n",
    "MASK_HEART_DIR = os.path.join(MASK_DIR, \"heart\")\n",
    "MASK_HEART_RESIZE_DIR = os.path.join(MASK_HEART_DIR, \"heart_resize\")\n",
    "\n",
    "os.makedirs(MASK_HEART_DIR, exist_ok=True)\n",
    "heart_mask = [os.path.join(MASK_HEART_DIR, gif) for gif in os.listdir(MASK_HEART_DIR)]\n",
    "heart_mask_resize = [path.replace(\"heart\", \"heart_resized\") for path in heart_mask]\n",
    "\n",
    "for src_img_path, dst_img_path in zip(heart_mask, heart_mask_resize):\n",
    "    img = cv2.resize(plt.imread(src_img_path) , dsize=(512, 512), interpolation=cv2.INTER_AREA)\n",
    "    plt.imsave(dst_img_path, img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "NODULE_PNG_DIR = os.path.join(DATA_DIR, \"JSRT\", \"nodules\", \"png\")\n",
    "NON_NODULE_PNG_DIR = os.path.join(DATA_DIR, \"JSRT\", \"non_nodules\", \"png\")\n",
    "nodules_pngs = [os.path.join(NODULE_PNG_DIR, png) for png in os.listdir(NODULE_PNG_DIR)]\n",
    "non_nodules_pngs = [os.path.join(NON_NODULE_PNG_DIR, png) for png in os.listdir(NON_NODULE_PNG_DIR)]\n",
    "entire_img = nodules_pngs + non_nodules_pngs\n",
    "heart_mask_resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "samples = [os.path.basename(path).replace(\".gif\", \"\") for path in heart_mask_resize]\n",
    "train_samples, test_samples = train_test_split(samples)\n",
    "\n",
    "for train_sample in train_samples:\n",
    "    for img in entire_img:\n",
    "        if train_sample not in img:\n",
    "            continue\n",
    "\n",
    "        shutil.copy(img, os.path.join(TRAIN_DIR, \"img\", f\"{train_sample}.png\"))\n",
    "    for mask in heart_mask_resize:\n",
    "        if train_sample not in mask:\n",
    "            continue\n",
    "\n",
    "        shutil.copy(mask, os.path.join(TRAIN_DIR, \"mask\", f\"{train_sample}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_sample in test_samples:\n",
    "    for img in entire_img:\n",
    "        if test_sample not in img:\n",
    "            continue\n",
    "\n",
    "        shutil.copy(img, os.path.join(TEST_DIR, \"img\", f\"{test_sample}.png\"))\n",
    "    for mask in heart_mask_resize:\n",
    "        if test_sample not in mask:\n",
    "            continue\n",
    "\n",
    "        shutil.copy(mask, os.path.join(TEST_DIR, \"mask\", f\"{test_sample}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_DIR = os.path.join(TRAIN_DIR, \"img\")\n",
    "TRAIN_MASK_DIR = os.path.join(TRAIN_DIR, \"mask\")\n",
    "train_imgs = [os.path.join(TRAIN_IMG_DIR, png) for png in os.listdir(TRAIN_IMG_DIR)]\n",
    "train_masks = [os.path.join(TRAIN_MASK_DIR, png) for png in os.listdir(TRAIN_MASK_DIR)]\n",
    "\n",
    "TEST_IMG_DIR = os.path.join(TEST_DIR, \"img\")\n",
    "TEST_MASK_DIR = os.path.join(TEST_DIR, \"mask\")\n",
    "test_imgs = [os.path.join(TEST_IMG_DIR, png) for png in os.listdir(TEST_IMG_DIR)]\n",
    "test_masks = [os.path.join(TEST_MASK_DIR, png) for png in os.listdir(TEST_MASK_DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_files = tf.constant(train_imgs)\n",
    "train_mask_files = tf.constant(train_masks)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_img_files, train_mask_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(image_path, mask_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=3)\n",
    "    mask = tf.math.reduce_max(mask, axis=-1, keepdims=True)\n",
    "    return img, mask\n",
    "\n",
    "def preprocess(image, mask):\n",
    "    input_image = tf.image.resize(image, (512, 512), method='nearest')\n",
    "    input_mask = tf.image.resize(mask, (512, 512), method='nearest')\n",
    "\n",
    "    return input_image, input_mask/255\n",
    "\n",
    "#Train dataset\n",
    "image_ds = dataset.map(process_path)\n",
    "train_processed_image_ds = image_ds.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(512, 512, 3), dtype=tf.float32, name=None), TensorSpec(shape=(512, 512, 1), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "VAL_SUBSPLITS = 3\n",
    "BUFFER_SIZE = 500\n",
    "BATCH_SIZE = 2\n",
    "train_processed_image_ds.batch(BATCH_SIZE)\n",
    "train_dataset = train_processed_image_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "print(train_processed_image_ds.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6213e0af081d56db034f795ff49d96980936e2ae540dda57fc6c3cbea6a5fc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
